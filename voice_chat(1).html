<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>语音聊天助手</title>
  <!-- 引入Element Plus CSS -->
  <link rel="stylesheet" href="https://unpkg.com/element-plus/dist/index.css">
  <!-- 引入Vue 3和Element Plus JS -->
  <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
  <script src="https://unpkg.com/element-plus"></script>
  <!-- 引入Element Plus Icons -->
  <script src="https://unpkg.com/@element-plus/icons-vue"></script>

  <style>
    /* General Styles */
    body,
    html {
      margin: 0;
      padding: 0;
      height: 100%;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
      background-color: #f4f7f6;
      /* Slightly off-white background */
    }

    #app {
      width: 100%;
      height: 100%;
      display: flex;
      justify-content: center;
      /* Center the main container */
      padding-top: 20px;
      /* Add some space at the top */
      box-sizing: border-box;
    }

    /* VoiceChatView Styles */
    .voice-chat-view {
      width: 100%;
      max-width: 800px;
      /* Limit max width */
      height: calc(100% - 40px);
      /* Adjust height considering padding */
      display: flex;
      flex-direction: column;
      position: relative;
      background-color: #fff;
      /* White background for the view */
      border-radius: 8px;
      /* Rounded corners */
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      /* Add shadow */
      overflow: hidden;
      /* Hide overflow */
    }

    .voice-chat-view h2 {
      text-align: center;
      padding: 15px 0;
      margin: 0;
      color: #303133;
      font-weight: 500;
      border-bottom: 1px solid #e4e7ed;
      /* Separator line */
    }

    /* Emotion Panel Styles */
    .emotion-panel {
      background-color: #f9f9f9;
      border-bottom: 1px solid #e4e7ed;
      overflow: hidden;
      transition: max-height 0.3s ease-out;
      max-height: 500px;
      /* Adjust as needed */
    }

    .emotion-panel.collapsed {
      max-height: 0;
      padding-top: 0;
      padding-bottom: 0;
      border-bottom: none;
    }

    .emotion-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 8px 15px;
      background-color: #e9f4fe;
      border-bottom: 1px solid #e0e0e0;
      cursor: pointer;
      /* Make header clickable */
    }

    .emotion-header h3 {
      margin: 0;
      font-size: 16px;
      color: #409eff;
    }

    .toggle-button {
      /* Style for the inline toggle button */
      background-color: #ecf5ff;
      border: 1px solid #d9ecff;
      color: #409eff;
      padding: 4px 8px;
      border-radius: 4px;
      cursor: pointer;
      font-size: 12px;
    }

    .toggle-button:hover {
      background-color: #d9ecff;
    }

    .emotion-content {
      padding: 15px;
      /* max-height: 400px; /* Limit height */
      /* overflow-y: auto; */
      /* Allow scrolling if needed */
    }

    .emotion-camera {
      margin-bottom: 15px;
      width: 100%;
    }

    .emotion-info {
      display: flex;
      align-items: center;
      margin-bottom: 15px;
      margin-top: 10px;
    }

    .emotion-info .label {
      font-weight: bold;
      margin-right: 10px;
      color: #606266;
    }

    .emotion-info .value {
      font-size: 16px;
      font-weight: bold;
      padding: 3px 10px;
      border-radius: 4px;
    }

    .emotion-distribution h4 {
      margin: 15px 0 10px 0;
      font-size: 14px;
      color: #606266;
    }

    .emotion-bars {
      max-height: 150px;
      /* Adjust height */
      overflow-y: auto;
    }

    .emotion-bar {
      display: flex;
      align-items: center;
      margin-bottom: 6px;
    }

    .bar-label {
      width: 60px;
      font-size: 13px;
      text-align: right;
      padding-right: 10px;
      color: #606266;
      flex-shrink: 0;
    }

    .bar-container {
      flex: 1;
      height: 18px;
      /* Slightly smaller bar */
      background-color: #eee;
      border-radius: 9px;
      position: relative;
      overflow: hidden;
      display: flex;
      align-items: center;
    }

    .bar {
      height: 100%;
      border-radius: 9px;
      transition: width 0.3s;
    }

    .bar-value {
      position: absolute;
      /* Position value inside bar */
      right: 8px;
      font-size: 11px;
      color: #fff;
      /* White text on colored bars */
      font-weight: bold;
      text-shadow: 1px 1px 1px rgba(0, 0, 0, 0.3);
      /* Add shadow for readability */
    }

    /* Emotion Colors (Used by panel value and bars) */
    .happy {
      color: #67c23a;
      background-color: #f0f9eb;
    }

    .sad {
      color: #909399;
      background-color: #f4f4f5;
    }

    .angry {
      color: #f56c6c;
      background-color: #fef0f0;
    }

    .fearful {
      color: #e6a23c;
      background-color: #fdf6ec;
    }

    .disgusted {
      color: #9932cc;
      background-color: #f9ecff;
    }

    .surprised {
      color: #409eff;
      background-color: #ecf5ff;
    }

    .neutral {
      color: #606266;
      background-color: #f5f7fa;
    }

    .unknown {
      color: #c0c4cc;
      background-color: #fafafa;
    }

    .bar.happy {
      background-color: #67c23a;
    }

    .bar.sad {
      background-color: #909399;
    }

    .bar.angry {
      background-color: #f56c6c;
    }

    .bar.fearful {
      background-color: #e6a23c;
    }

    .bar.disgusted {
      background-color: #9932cc;
    }

    .bar.surprised {
      background-color: #409eff;
    }

    .bar.neutral {
      background-color: #606266;
    }

    .bar.unknown {
      background-color: #c0c4cc;
    }


    /* VoiceChat Component Styles */
    .voice-chat-component {
      /* Add wrapper class */
      flex: 1;
      display: flex;
      flex-direction: column;
      overflow: hidden;
      /* Prevent controls from overflowing */
    }

    .chat-container {
      flex: 1;
      display: flex;
      flex-direction: column;
      /* border: 1px solid #dcdfe6; */
      /* Removed border as view has one */
      /* border-radius: 4px; */
      overflow: hidden;
      background-color: #fff;
      height: 100%;
      /* Ensure it takes available height */
    }

    .messages {
      flex: 1;
      overflow-y: auto;
      padding: 20px;
      background-color: #f5f7fa;
    }

    .message {
      margin-bottom: 20px;
      display: flex;
      flex-direction: column;
    }

    .message.user {
      align-items: flex-end;
    }

    .message.assistant {
      align-items: flex-start;
    }

    .message.error {
      align-items: center;
    }

    .message-content {
      max-width: 80%;
      padding: 10px 15px;
      /* Adjusted padding */
      border-radius: 12px;
      /* More rounded */
      box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
      word-wrap: break-word;
      /* Ensure long words break */
    }

    .message.user .message-content {
      background-color: #ecf5ff;
      color: #409eff;
    }

    .message.assistant .message-content {
      background-color: #f0f2f5;
      color: #303133;
    }

    /* Slightly different assistant color */
    .message.error .message-content {
      background-color: #fef0f0;
      color: #f56c6c;
    }

    .message-audio {
      margin-top: 8px;
    }

    .message-audio audio {
      width: 100%;
      max-width: 300px;
      height: 40px;
    }

    /* Control audio size */

    .controls {
      padding: 12px 16px;
      /* Adjusted padding */
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 12px;
      /* Adjusted gap */
      background-color: #fff;
      border-top: 1px solid #e4e7ed;
      flex-wrap: wrap;
      flex-shrink: 0;
      /* Prevent controls from shrinking */
    }

    /* Make icons align better in buttons */
    .controls .el-button .el-icon {
      vertical-align: middle;
      margin-right: 4px;
      /* Space between icon and text */
    }

    /* Debug Info Styles */
    .debug-info {
      padding: 10px 16px;
      background-color: #f8f9fa;
      border-top: 1px solid #e4e7ed;
      max-height: 200px;
      /* Reduced max height */
      overflow-y: auto;
      font-size: 12px;
      /* Smaller font */
      flex-shrink: 0;
      /* Prevent debug from shrinking */
    }

    .debug-info h3 {
      margin-top: 0;
      margin-bottom: 8px;
      font-size: 14px;
      color: #606266;
    }

    .debug-item {
      display: flex;
      margin-bottom: 6px;
    }

    .debug-item .label {
      width: 110px;
      font-weight: bold;
      color: #606266;
      flex-shrink: 0;
    }

    .debug-item .status {
      flex: 1;
      word-break: break-all;
    }

    .debug-item .status.idle {
      color: #909399;
    }

    .debug-item .status.connected,
    .debug-item .status.recording,
    .debug-item .status.processing,
    .debug-item .status.success {
      color: #67c23a;
    }

    .debug-item .status.error,
    .debug-item .status.disconnected {
      color: #f56c6c;
    }

    .debug-item .error {
      color: #f56c6c;
      word-break: break-all;
    }

    .debug-item .asr-result {
      color: #409eff;
      font-weight: bold;
      word-break: break-all;
    }

    .debug-item .raw-message {
      background-color: #eee;
      padding: 5px;
      border-radius: 4px;
      overflow: auto;
      max-height: 100px;
      margin: 0;
      color: #555;
      font-family: monospace;
      word-break: break-all;
      white-space: pre-wrap;
    }


    /* EmotionCamera Component Styles */
    .emotion-camera-component {
      /* Add wrapper class */
      width: 100%;
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .camera-container {
      width: 100%;
      max-width: 480px;
      /* Smaller max width */
      aspect-ratio: 4/3;
      border-radius: 8px;
      overflow: hidden;
      position: relative;
      background-color: #2c3e50;
      /* Darker background */
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);
      margin: 0 auto 10px auto;
      /* Center and add margin */
    }

    .camera-preview {
      display: block;
      /* Remove extra space below video */
      width: 100%;
      height: 100%;
      object-fit: cover;
      opacity: 0;
      transition: opacity 0.5s ease-in-out;
      transform: scaleX(-1);
      /* Mirror view */
    }

    .camera-preview.active {
      opacity: 1;
    }

    .hidden-canvas {
      display: none;
    }

    .camera-status {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      color: #909399;
      background-color: rgba(0, 0, 0, 0.1);
      /* Slight overlay */
    }

    .status-icon {
      margin-bottom: 10px;
    }

    /* Reduced margin */
    .status-text {
      font-size: 14px;
    }

    .camera-overlay {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0, 0, 0, 0.4);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      color: #fff;
      z-index: 2;
      opacity: 0;
      visibility: hidden;
      transition: opacity 0.3s, visibility 0.3s;
    }

    .camera-overlay.visible {
      opacity: 1;
      visibility: visible;
    }

    /* Control visibility */
    .processing-icon {
      margin-bottom: 10px;
      animation: spin 1s linear infinite;
    }

    @keyframes spin {
      0% {
        transform: rotate(0deg);
      }

      100% {
        transform: rotate(360deg);
      }
    }

    .processing-text {
      font-size: 14px;
    }

    .emotion-result {
      position: absolute;
      top: 10px;
      right: 10px;
      z-index: 1;
      transition: opacity 0.3s, transform 0.3s;
      opacity: 0;
      transform: translateY(-10px);
    }

    .emotion-result.visible {
      opacity: 1;
      transform: translateY(0);
    }

    /* Animate in */

    .emotion-badge {
      padding: 4px 12px;
      border-radius: 16px;
      font-weight: bold;
      font-size: 13px;
      color: #fff;
      background-color: rgba(0, 0, 0, 0.6);
      box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2);
    }

    /* Use background only for badges */
    .emotion-badge.happy {
      background-color: #67c23a;
    }

    .emotion-badge.sad {
      background-color: #909399;
    }

    .emotion-badge.angry {
      background-color: #f56c6c;
    }

    .emotion-badge.fearful {
      background-color: #e6a23c;
    }

    .emotion-badge.disgusted {
      background-color: #9932cc;
    }

    .emotion-badge.surprised {
      background-color: #409eff;
    }

    .emotion-badge.neutral {
      background-color: #606266;
    }

    .emotion-badge.unknown {
      background-color: #c0c4cc;
    }

    .camera-controls {
      margin-top: 0px;
      /* No extra margin needed now */
      text-align: center;
      /* Center button */
    }

    .camera-controls .el-button {
      min-width: 140px;
      /* Ensure button has min width */
    }

    /* Responsive adjustments */
    @media (max-width: 600px) {
      .voice-chat-view {
        max-width: 100%;
        height: 100%;
        border-radius: 0;
      }

      .message-content {
        max-width: 90%;
      }

      .controls {
        gap: 8px;
      }

      .debug-item {
        flex-direction: column;
      }

      .debug-item .label {
        width: 100%;
        margin-bottom: 4px;
      }
    }
  </style>
</head>

<body>
  <div id="app">
    <!-- Main View Structure (from VoiceChatView) -->
    <div class="voice-chat-view">
      <h2>语音聊天助手</h2>

      <!-- Emotion Panel -->
      <div class="emotion-panel" :class="{ collapsed: !showEmotionPanel }">
        <div class="emotion-header" @click="showEmotionPanel = !showEmotionPanel">
          <h3>情绪识别信息</h3>
          <!-- Toggle button removed from header, click header instead -->
        </div>
        <div class="emotion-content">
          <!-- Emotion Camera Component -->
          <emotion-camera ref="emotionCameraRef" :auto-start="false" :capture-interval="2000"
            @emotion-detected="updateEmotion" @camera-status-change="onCameraStatusChange" />
          <!-- Emotion Info -->
          <div class="emotion-info">
            <span class="label">主要情绪:</span>
            <span class="value" :class="currentEmotion.value || 'unknown'">{{ emotionDisplay }}</span>
          </div>
          <!-- Emotion Distribution -->
          <div class="emotion-distribution" v-if="currentEmotionData?.emotion_distribution">
            <h4>情绪分布</h4>
            <div class="emotion-bars">
              <div v-for="(value, key) in currentEmotionData.emotion_distribution" :key="key" class="emotion-bar">
                <div class="bar-label">{{ translateEmotion(key) }}</div>
                <div class="bar-container">
                  <div class="bar" :style="{ width: `${value*100}%` }" :class="key"></div>
                  <span class="bar-value">{{ (value*100).toFixed(1) }}%</span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- Voice Chat Component -->
      <voice-chat class="voice-chat-component"></voice-chat>

      <!-- Floating toggle removed, click header to toggle -->
    </div>
  </div>

  <script>
    const { createApp, ref, onMounted, onUnmounted, nextTick, computed } = Vue;
    const { ElMessage, ElButton, ElIcon } = ElementPlus; // Import necessary Element Plus components

    // 1. Define EmotionCamera Component
    const EmotionCameraComponent = {
      props: {
        autoStart: { type: Boolean, default: false },
        captureInterval: { type: Number, default: 2000 }
      },
      emits: ['emotion-detected', 'camera-status-change'],
      template: `
        <div class="emotion-camera-component">
          <div class="camera-controls">
             <el-button
              :type="isActive ? 'danger' : 'primary'"
              @click="toggleCamera"
              :loading="starting"
            >
              <el-icon><component :is="isActive ? 'VideoPause' : 'VideoCamera'" /></el-icon>
              {{ isActive ? '关闭摄像头' : '开启摄像头' }}
            </el-button>
          </div>
          <div class="camera-container">
            <video ref="videoRef" class="camera-preview" :class="{ active: isActive }" autoplay muted playsinline></video>
            <canvas ref="canvasRef" class="hidden-canvas"></canvas>
            <div class="camera-status" v-if="!isActive">
              <div class="status-icon"><el-icon :size="48"><VideoCamera /></el-icon></div>
              <div class="status-text">摄像头未启动</div>
            </div>
            <div class="camera-overlay" :class="{ visible: processing }">
              <div class="processing-icon"><el-icon :size="36" class="is-loading"><Loading /></el-icon></div>
              <div class="processing-text">分析中...</div>
            </div>
            <div class="emotion-result" :class="{ visible: currentEmotion && isActive }">
              <div v-if="currentEmotion" class="emotion-badge" :class="currentEmotion">
                {{ translateEmotion(currentEmotion) }}
              </div>
            </div>
          </div>
        </div>
      `,
      setup(props, { emit }) {
        const videoRef = ref(null);
        const canvasRef = ref(null);
        const isActive = ref(false);
        const stream = ref(null);
        const captureTimer = ref(null);
        const processing = ref(false);
        const starting = ref(false); // Loading state for button
        const currentEmotion = ref(null);

        const emotionTranslations = { 'angry': '愤怒', 'disgusted': '厌恶', 'fearful': '恐惧', 'happy': '高兴', 'sad': '悲伤', 'surprised': '惊讶', 'neutral': '平静', 'unknown': '未知' };
        const translateEmotion = (key) => emotionTranslations[key] || key;

        const startCamera = async () => {
          if (isActive.value || starting.value) return;
          starting.value = true;
          console.log('尝试启动摄像头...');
          try {
            stream.value = await navigator.mediaDevices.getUserMedia({ video: { width: { ideal: 640 }, height: { ideal: 480 }, facingMode: 'user' }, audio: false });
            console.log('摄像头权限已获取');
            if (videoRef.value) {
              videoRef.value.srcObject = stream.value;
              videoRef.value.onloadedmetadata = () => {
                // Use play() promise to ensure video starts playing
                videoRef.value.play().then(() => {
                  console.log('视频元素已开始播放');
                  isActive.value = true;
                  starting.value = false;
                  emit('camera-status-change', true);
                  startFrameCapture();
                }).catch(error => {
                  console.error('视频播放失败:', error);
                  ElMessage.error('无法播放摄像头视频');
                  stopCamera(); // Clean up if play fails
                  starting.value = false;
                });
              };
              videoRef.value.onerror = (e) => {
                console.error("视频元素错误:", e);
                ElMessage.error("摄像头视频流出错");
                stopCamera();
                starting.value = false;
              };
            } else {
              starting.value = false;
            }
          } catch (error) {
            console.error('无法访问摄像头:', error);
            let errorMsg = '无法访问摄像头';
            if (error.name === 'NotAllowedError') errorMsg = '请允许浏览器访问您的摄像头';
            else if (error.name === 'NotFoundError') errorMsg = '未检测到摄像头设备';
            else if (error.name === 'NotReadableError') errorMsg = '摄像头已被占用或硬件错误';
            else errorMsg = `摄像头错误: ${error.name}`;
            ElMessage.error(errorMsg);
            starting.value = false;
          }
        };

        const stopCamera = () => {
          console.log("停止摄像头...");
          stopFrameCapture();
          if (stream.value) {
            stream.value.getTracks().forEach(track => track.stop());
            stream.value = null;
            console.log("视频流已停止");
          }
          if (videoRef.value && videoRef.value.srcObject) {
            videoRef.value.srcObject = null;
            console.log("视频源已清除");
          }
          isActive.value = false;
          currentEmotion.value = null; // Reset emotion on stop
          emit('camera-status-change', false);
          starting.value = false; // Ensure starting state is reset
        };

        const toggleCamera = () => {
          if (isActive.value) stopCamera();
          else startCamera();
        };

        const startFrameCapture = () => {
          stopFrameCapture(); // Clear existing timer
          console.log(`开始捕获帧，间隔: ${props.captureInterval}ms`);
          captureTimer.value = setInterval(captureFrame, props.captureInterval);
        };

        const stopFrameCapture = () => {
          if (captureTimer.value) {
            clearInterval(captureTimer.value);
            captureTimer.value = null;
            console.log("停止捕获帧");
          }
        };

        const captureFrame = async () => {
          if (!isActive.value || !videoRef.value || !canvasRef.value || processing.value || !stream.value) {
            // console.log("捕获条件不满足，跳过");
            return;
          }

          const video = videoRef.value;
          if (video.readyState < video.HAVE_METADATA || video.videoWidth === 0) {
            // console.log('视频元素尚未准备好捕获');
            return;
          }

          processing.value = true; // Set processing before async operation
          // console.log("开始捕获和处理帧...");

          try {
            const canvas = canvasRef.value;
            const context = canvas.getContext('2d');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;

            // Mirror the captured frame since the preview is mirrored
            context.translate(canvas.width, 0);
            context.scale(-1, 1);
            context.drawImage(video, 0, 0, canvas.width, canvas.height);
            context.setTransform(1, 0, 0, 1, 0, 0); // Reset transform

            const imageData = canvas.toDataURL('image/jpeg', 0.8);
            await sendFrameToBackend(imageData);
          } catch (error) {
            console.error('捕获视频帧失败:', error);
            processing.value = false; // Reset processing on error
          } finally {
            // processing.value = false; // Moved to sendFrameToBackend finally block
          }
        };

        const sendFrameToBackend = async (imageData) => {
          // console.log("发送帧到后端...");
          try {
            // 修改: 使用绝对 URL
            const response = await fetch('http://localhost:5000/api/emotion/detect', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ image: imageData.split(',')[1] })
            });
            if (!response.ok) {
              throw new Error(`服务器错误: ${response.status}`);
            }
            const data = await response.json();
            // console.log("收到情绪结果:", data);
            if (data.status === 'success' && data.emotion) {
              currentEmotion.value = data.emotion;
              emit('emotion-detected', {
                emotion: data.emotion,
                emotion_distribution: data.emotion_distribution || null
              });
            } else if (data.message === '未检测到人脸') {
              // console.log("后端未检测到人脸");
              currentEmotion.value = null; // Clear emotion if no face detected
              // Optionally emit neutral or null emotion? For now, just clear local display.
              emit('emotion-detected', { emotion: 'neutral', emotion_distribution: null, no_face: true }); // Indicate no face
            }
            else {
              console.warn('情绪识别API返回非成功状态:', data);
              currentEmotion.value = 'unknown'; // Show unknown on API error
              emit('emotion-detected', { emotion: 'unknown', emotion_distribution: null });
            }
          } catch (error) {
            console.error('发送视频帧到后端失败:', error);
            currentEmotion.value = 'unknown'; // Show unknown on fetch error
            emit('emotion-detected', { emotion: 'unknown', emotion_distribution: null });
            // Avoid flooding with messages ElMessage.error('情绪分析连接失败');
          } finally {
            processing.value = false; // Reset processing after fetch completes
            // console.log("帧处理完成");
          }
        };

        onMounted(() => {
          console.log('EmotionCamera 组件已挂载');
          if (props.autoStart) startCamera();
        });

        onUnmounted(() => {
          console.log('EmotionCamera 组件卸载');
          stopCamera(); // Ensure camera and timers are stopped
        });

        // Expose methods if needed by parent (currently not used)
        // defineExpose({ startCamera, stopCamera });

        return { videoRef, canvasRef, isActive, processing, currentEmotion, starting, toggleCamera, translateEmotion };
      }
    };

    // 2. Define VoiceChat Component
    const VoiceChatComponent = {
      template: `
        <div class="chat-container">
          <div class="messages" ref="messagesContainer">
            <div v-for="(message, index) in messages" :key="index" :class="['message', message.type]">
              <div class="message-content">
                <div class="message-text">{{ message.text }}</div>
                <div v-if="message.audio" class="message-audio">
                  <audio :src="message.audio" controls></audio>
                </div>
              </div>
            </div>
          </div>

          <div class="debug-info" v-if="debugInfo.show">
             <h3>调试信息</h3>
             <div class="debug-item"><span class="label">WebSocket状态:</span><span :class="['status', debugInfo.wsStatus]">{{ debugInfo.wsStatus }}</span></div>
             <div class="debug-item"><span class="label">语音识别状态:</span><span :class="['status', debugInfo.asrStatus]">{{ debugInfo.asrStatus }}</span></div>
             <div class="debug-item" v-if="debugInfo.asrResult"><span class="label">ASR结果:</span><span class="asr-result">{{ debugInfo.asrResult }}</span></div>
             <div class="debug-item"><span class="label">LLM/RAG状态:</span><span :class="['status', debugInfo.ragStatus]">{{ debugInfo.ragStatus }}</span></div>
             <div class="debug-item"><span class="label">TTS状态:</span><span :class="['status', debugInfo.ttsStatus]">{{ debugInfo.ttsStatus }}</span></div>
             <div class="debug-item" v-if="debugInfo.error"><span class="label">错误信息:</span><span class="error">{{ debugInfo.error }}</span></div>
             <div class="debug-item" v-if="debugInfo.rawMessage"><span class="label">原始WebSocket消息:</span><pre class="raw-message">{{ debugInfo.rawMessage }}</pre></div>
          </div>

          <div class="controls">
            <el-button type="primary" :loading="isProcessing" @click="toggleRecording">
              <el-icon><component :is="isRecording ? 'Microphone' : 'Microphone'" /></el-icon> <!-- Always Microphone icon -->
              {{ isRecording ? '停止录音' : (isProcessing ? '处理中...' : '开始录音') }}
            </el-button>
            <el-button v-if="isRecording" type="danger" @click="cancelRecording" :disabled="isProcessing">
              <el-icon><Close /></el-icon>
              取消
            </el-button>
            <el-button type="info" @click="toggleDebug" :disabled="isProcessing">
              <el-icon><ChatDotRound /></el-icon>
              {{ debugInfo.show ? '隐藏调试' : '显示调试' }}
            </el-button>
          </div>
        </div>
      `,
      setup() {
        const messages = ref([]);
        const isRecording = ref(false);
        const isProcessing = ref(false);
        const mediaRecorder = ref(null); // Stores { stream, scriptNode, audioContext, stop }
        const ws = ref(null);
        const messagesContainer = ref(null);
        const silenceTimer = ref(null);
        const audioContext = ref(null); // Store audio context reference


        const debugInfo = ref({
          show: false, wsStatus: 'idle', asrStatus: 'idle', ragStatus: 'idle',
          ttsStatus: 'idle', asrResult: null, error: null, rawMessage: null
        });

        const SILENCE_TIMEOUT = 1500;

        const clearSilenceTimer = () => {
          if (silenceTimer.value) clearTimeout(silenceTimer.value);
          silenceTimer.value = null;
        };

        const startSilenceTimer = () => {
          clearSilenceTimer();
          silenceTimer.value = setTimeout(() => {
            if (isRecording.value) {
              console.log('静音超时，自动停止录音...');
              ElMessage.info('检测到静音，自动结束');
              stopRecording();
            }
          }, SILENCE_TIMEOUT);
        };

        const scrollToBottom = () => {
          nextTick(() => {
            if (messagesContainer.value) {
              messagesContainer.value.scrollTop = messagesContainer.value.scrollHeight;
            }
          });
        };

        const initWebSocket = () => {
          debugInfo.value = { ...debugInfo.value, wsStatus: 'connecting', error: null };
          try {
            ws.value = new WebSocket('ws://localhost:8091/paddlespeech/asr/streaming');
            console.log("Attempting to connect to WebSocket...");

            ws.value.onopen = () => {
              console.log('ASR WebSocket 已连接');
              debugInfo.value.wsStatus = 'connected';
              ElMessage.success('语音识别服务已连接');
            };

            ws.value.onmessage = (event) => {
              // console.log("Raw WS message:", event.data); // Log raw messages
              try {
                const data = JSON.parse(event.data);
                debugInfo.value.rawMessage = JSON.stringify(data, null, 2);

                if (isRecording.value && data.result !== undefined) {
                  debugInfo.value.asrResult = data.result; // Update intermediate results
                  // Reset silence timer on receiving intermediate result
                  if (data.result) startSilenceTimer();
                }

                // Check for final result (PaddleSpeech doesn't have a clear 'final' flag in streaming)
                // We rely on the state transition: !isRecording && isProcessing
                if (!isRecording.value && isProcessing.value) {
                  if (data.result !== undefined) { // Got a result after sending 'end'
                    const asrText = data.result || '';
                    console.log('[调试] 收到可能的最终 ASR 结果:', asrText);
                    debugInfo.value.asrStatus = 'success';
                    debugInfo.value.asrResult = asrText;

                    if (asrText && asrText.trim()) {
                      messages.value.push({ type: 'user', text: asrText });
                      scrollToBottom();
                      callDeepSeekAPI(asrText); // Call LLM
                    } else {
                      console.log('ASR 最终结果为空');
                      ElMessage.warning('未能识别到有效语音');
                      isProcessing.value = false; // Reset processing state
                      debugInfo.value.asrStatus = 'idle';
                    }
                  } else if (data.status && data.status !== 0) { // Error status from PaddleSpeech
                    console.error('ASR 服务器错误状态:', data);
                    ElMessage.error('语音识别失败: ' + (data.message || '未知错误'));
                    debugInfo.value.error = `ASR 错误: ${data.status} - ${data.message || JSON.stringify(data)}`;
                    debugInfo.value.asrStatus = 'error';
                    isProcessing.value = false;
                  } else {
                    // Sometimes the final message might be empty or lack 'result' after 'end' signal
                    console.warn('收到非预期的最终 ASR 消息:', data, '使用上一个中间结果 (如果存在)');
                    const lastAsrText = debugInfo.value.asrResult || '';
                    if (lastAsrText && lastAsrText.trim()) {
                      messages.value.push({ type: 'user', text: lastAsrText });
                      scrollToBottom();
                      callDeepSeekAPI(lastAsrText);
                    } else {
                      ElMessage.warning('未能识别到有效语音');
                      isProcessing.value = false;
                      debugInfo.value.asrStatus = 'idle';
                    }
                  }
                }

              } catch (error) {
                console.error('处理 ASR 消息失败:', error, "Data:", event.data);
                debugInfo.value.error = '处理 ASR 消息失败: ' + error.message;
                if (!isRecording.value && isProcessing.value) {
                  isProcessing.value = false;
                  debugInfo.value.asrStatus = 'error';
                }
              }
            };

            ws.value.onerror = (error) => {
              console.error('ASR WebSocket 错误:', error);
              debugInfo.value = { ...debugInfo.value, error: 'ASR WebSocket 连接错误', wsStatus: 'error', asrStatus: 'error' };
              ElMessage.error('语音识别服务连接错误');
              isProcessing.value = false;
              isRecording.value = false;
            };

            ws.value.onclose = (event) => {
              console.log(`ASR WebSocket 关闭. Code: ${event.code}, Reason: ${event.reason}`);
              // Prevent state changes if it was closed intentionally
              if (isRecording.value || isProcessing.value) {
                ElMessage.warning('语音识别连接意外断开');
                debugInfo.value.error = `ASR WebSocket 意外断开 (${event.code})`;
              }
              debugInfo.value = { ...debugInfo.value, wsStatus: 'disconnected', asrStatus: 'idle' };
              isProcessing.value = false;
              isRecording.value = false;
              // Attempt to reconnect? Maybe add a delay.
              // setTimeout(initWebSocket, 5000);
            };
          } catch (error) {
            console.error('初始化 ASR WebSocket 失败:', error);
            debugInfo.value = { ...debugInfo.value, error: '初始化 WebSocket 失败', wsStatus: 'error' };
            ElMessage.error('无法连接语音识别服务');
          }
        };

        const callDeepSeekAPI = async (text) => {
          if (!text || !text.trim()) {
            console.warn("LLM 调用被跳过，因为 ASR 文本为空");
            isProcessing.value = false; // Reset processing if ASR was empty
            debugInfo.value.asrStatus = 'idle';
            return;
          }
          debugInfo.value.ragStatus = 'processing';
          console.log('调用后端 LLM，输入:', text.substring(0, 50) + "...");
          try {
            // 修改: 使用绝对 URL
            const response = await fetch(`http://localhost:5000/api/llm/deepseek_chat`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ query: text })
            });

            if (!response.ok) {
              let errorMsg = '未知错误';
              try {
                const errorData = await response.json();
                errorMsg = errorData.message || JSON.stringify(errorData);
              } catch (e) {
                errorMsg = await response.text();
              }
              throw new Error(`LLM 代理错误 ${response.status}: ${errorMsg}`);
            }

            const data = await response.json();
            debugInfo.value.ragStatus = 'success';

            if (data.summarized_emotion) {
              console.log('[调试] 收到总结情绪:', data.summarized_emotion);
              // Log or use this summarized emotion if needed elsewhere
            }

            if (data.status === 'success' && data.response) {
              const aiResponse = data.response;
              console.log('LLM 回复:', aiResponse.substring(0, 50) + "...");
              messages.value.push({ type: 'assistant', text: aiResponse });
              scrollToBottom();
              await textToSpeech(aiResponse); // TTS will set isProcessing=false in its finally block
            } else {
              throw new Error(data.message || 'LLM 代理返回失败状态');
            }
          } catch (error) {
            console.error('调用后端 LLM 失败:', error);
            ElMessage.error('获取 AI 回复失败: ' + error.message);
            debugInfo.value = { ...debugInfo.value, error: 'LLM 调用失败: ' + error.message, ragStatus: 'error' };
            messages.value.push({ type: 'error', text: '抱歉，AI 服务连接出错。' });
            scrollToBottom();
            isProcessing.value = false; // Reset processing on LLM error
          }
        };

        const textToSpeech = async (text) => {
          if (!text || !text.trim()) {
            console.warn("TTS 调用被跳过，因为文本为空");
            isProcessing.value = false; // Must reset processing state if TTS is skipped
            return;
          }
          debugInfo.value.ttsStatus = 'processing';
          console.log('请求 TTS，文本:', text.substring(0, 50) + "...");
          try {
            // 修改: 使用绝对 URL
            const response = await fetch(`http://localhost:5000/api/tts`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ text: text })
            });

            if (!response.ok) {
              const errorData = await response.text();
              throw new Error(`TTS 服务错误 ${response.status}: ${errorData}`);
            }

            const data = await response.json();
            if (data.status === 'success' && data.audio_url) {
              console.log('TTS 成功，URL:', data.audio_url);
              debugInfo.value.ttsStatus = 'success';
              const lastMessageIndex = messages.value.length - 1;
              if (lastMessageIndex >= 0 && messages.value[lastMessageIndex].type === 'assistant') {
                // Assume backend runs on localhost:5000 if relative URL is given
                const audioUrl = data.audio_url.startsWith('http') ? data.audio_url : `http://localhost:5000${data.audio_url}`;
                messages.value[lastMessageIndex].audio = audioUrl;
                await nextTick(); // Ensure DOM updates before potential autoplay
                // Find and play the audio? Be mindful of browser restrictions.
                const audioElements = messagesContainer.value?.querySelectorAll('audio');
                if (audioElements && audioElements.length > 0) {
                  const lastAudio = audioElements[audioElements.length - 1];
                  lastAudio.play().catch(e => console.warn("音频自动播放失败 (浏览器限制):", e));
                }
              }
            } else {
              throw new Error(data.message || 'TTS 未能生成音频 URL');
            }
          } catch (error) {
            console.error('调用 TTS 失败:', error);
            ElMessage.error('语音合成失败: ' + error.message);
            debugInfo.value = { ...debugInfo.value, error: 'TTS 调用失败: ' + error.message, ttsStatus: 'error' };
            // Don't add error message to chat for TTS failure, it's less critical
            // messages.value.push({ type: 'error', text: '抱歉，语音合成失败。' });
            // scrollToBottom();
          } finally {
            // IMPORTANT: Reset processing state only after TTS (or its failure) is complete
            isProcessing.value = false;
            debugInfo.value.asrStatus = 'idle'; // Reset ASR status too
            console.log('TTS/LLM 完整流程结束，isProcessing=false');
          }
        };

        const startRecording = async () => {
          if (isProcessing.value) {
            ElMessage.warning("请稍候，正在处理上一条语音...");
            return;
          }
          console.log("开始录音...");
          debugInfo.value = { ...debugInfo.value, error: null, asrStatus: 'recording', ragStatus: 'idle', ttsStatus: 'idle' };
          isRecording.value = true;
          // Reset potential previous ASR result
          debugInfo.value.asrResult = null;

          try {
            // Ensure WebSocket is connected
            if (!ws.value || ws.value.readyState !== WebSocket.OPEN) {
              ElMessage.info('正在重新连接语音服务...');
              initWebSocket(); // Attempt reconnect
              // Wait a short moment for connection
              await new Promise(resolve => setTimeout(resolve, 1000));
              if (!ws.value || ws.value.readyState !== WebSocket.OPEN) {
                throw new Error("无法连接到 ASR WebSocket");
              }
            }

            const stream = await navigator.mediaDevices.getUserMedia({ audio: { channelCount: 1, sampleRate: 16000, sampleSize: 16, echoCancellation: false, noiseSuppression: false, autoGainControl: false } });

            // Use existing AudioContext or create a new one
            if (!audioContext.value || audioContext.value.state === 'closed') {
              audioContext.value = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
            }
            // Resume context if suspended (often needed after user interaction)
            if (audioContext.value.state === 'suspended') {
              await audioContext.value.resume();
            }

            const audioSource = audioContext.value.createMediaStreamSource(stream);
            // Use a smaller buffer size for lower latency if needed, 4096 is standard
            const scriptNode = audioContext.value.createScriptProcessor(4096, 1, 1);

            scriptNode.onaudioprocess = (audioProcessingEvent) => {
              if (!isRecording.value) return; // Stop processing if recording stopped

              const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
              const pcmData = new Int16Array(inputData.length);
              for (let i = 0; i < inputData.length; i++) {
                pcmData[i] = Math.max(-1, Math.min(1, inputData[i])) * 32767; // 0x7FFF = 32767
              }

              if (ws.value && ws.value.readyState === WebSocket.OPEN) {
                ws.value.send(pcmData.buffer);
                startSilenceTimer(); // Reset silence timer on sending data
              } else {
                console.warn("WebSocket 未连接，无法发送音频数据");
                // Optionally stop recording if WS is lost mid-stream
                // stopRecording();
                // ElMessage.error("语音服务连接丢失");
              }
            };

            audioSource.connect(scriptNode);
            scriptNode.connect(audioContext.value.destination); // Connect to output (might not be necessary if just processing)

            // Send start signal to ASR WebSocket
            if (ws.value && ws.value.readyState === WebSocket.OPEN) {
              const startData = { name: `mic_${Date.now()}.wav`, signal: "start", nbest: 1 };
              ws.value.send(JSON.stringify(startData));
              console.log('ASR 开始信号已发送');
            }

            mediaRecorder.value = {
              stream, scriptNode, audioSource, stop: () => {
                clearSilenceTimer();
                stream.getTracks().forEach(track => track.stop());
                try {
                  // Disconnecting nodes can sometimes throw errors if context is closing
                  scriptNode.disconnect();
                  audioSource.disconnect();
                } catch (e) { console.warn("Error disconnecting audio nodes:", e); }
                // Don't close the context here, reuse it.
              }
            };

            startSilenceTimer(); // Start silence timer initially

          } catch (error) {
            console.error('录音启动失败:', error);
            debugInfo.value.error = '无法访问麦克风: ' + error.message;
            ElMessage.error('无法访问麦克风: ' + error.message);
            isRecording.value = false; // Reset state on error
            debugInfo.value.asrStatus = 'error';
          }
        };

        const stopRecording = () => {
          clearSilenceTimer();
          if (!mediaRecorder.value) {
            console.warn("停止录音调用，但 mediaRecorder 为空");
            isRecording.value = false;
            isProcessing.value = false;
            debugInfo.value.asrStatus = 'idle';
            return;
          }
          console.log("停止录音...");
          isRecording.value = false;
          isProcessing.value = true; // Set processing while waiting for final ASR
          debugInfo.value.asrStatus = 'processing';

          try {
            if (ws.value && ws.value.readyState === WebSocket.OPEN) {
              const endData = { signal: "end" }; // Name might not be required for 'end'
              ws.value.send(JSON.stringify(endData));
              console.log('ASR 结束信号已发送');
            } else {
              console.warn("WebSocket 未连接，无法发送结束信号");
              // If WS is closed, we won't get a final result, reset state
              isProcessing.value = false;
              debugInfo.value.asrStatus = 'error';
              debugInfo.value.error = "WebSocket 连接已关闭，无法获取结果";
            }
            mediaRecorder.value.stop(); // Stop audio processing and stream
            mediaRecorder.value = null;
            console.log("录音资源已清理");
          } catch (error) {
            console.error('停止录音过程中出错:', error);
            debugInfo.value.error = '停止录音失败: ' + error.message;
            isProcessing.value = false; // Reset state on error
            debugInfo.value.asrStatus = 'error';
          }
        };

        const cancelRecording = () => {
          clearSilenceTimer();
          if (mediaRecorder.value) {
            console.log("取消录音...");
            if (ws.value && ws.value.readyState === WebSocket.OPEN) {
              // PaddleSpeech doesn't have a 'cancel', send 'end' to terminate server processing
              const endData = { signal: "end" };
              ws.value.send(JSON.stringify(endData));
              console.log('ASR 取消(结束)信号已发送');
            }
            mediaRecorder.value.stop();
            mediaRecorder.value = null;
          }
          isRecording.value = false;
          isProcessing.value = false; // Reset processing immediately on cancel
          debugInfo.value.asrStatus = 'idle';
          debugInfo.value.asrResult = null; // Clear any intermediate result
          // window.speechSynthesis.cancel(); // Cancel any ongoing TTS
          console.log("录音已取消");
        };

        const toggleRecording = () => {
          if (isRecording.value) {
            stopRecording();
          } else if (!isProcessing.value) { // Prevent starting if already processing
            startRecording();
          } else {
            ElMessage.info("正在处理上一段语音，请稍候...");
          }
        };

        const toggleDebug = () => {
          debugInfo.value.show = !debugInfo.value.show;
        };

        onMounted(() => {
          console.log("VoiceChat 组件已挂载");
          initWebSocket();
          // Add initial message
          messages.value.push({ type: 'assistant', text: '你好！请点击"开始录音"按钮和我说话。' })
        });

        onUnmounted(() => {
          console.log("VoiceChat 组件卸载");
          clearSilenceTimer();
          if (ws.value) ws.value.close();
          if (mediaRecorder.value) mediaRecorder.value.stop();
          if (audioContext.value && audioContext.value.state !== 'closed') {
            audioContext.value.close().catch(e => console.warn("Error closing AudioContext:", e));
            audioContext.value = null;
          }
          // window.speechSynthesis.cancel();
        });

        return { messages, isRecording, isProcessing, debugInfo, messagesContainer, toggleRecording, cancelRecording, toggleDebug };
      }
    };

    // 3. Create Main Vue App
    const app = createApp({
      setup() {
        const showEmotionPanel = ref(true); // Default to showing the panel
        const currentEmotion = ref(null); // Stores { emotion: 'happy', distribution: {...} }
        const emotionCameraRef = ref(null); // Ref to access camera component methods if needed

        const emotionTranslations = { 'angry': '愤怒', 'disgusted': '厌恶', 'fearful': '恐惧', 'happy': '高兴', 'sad': '悲伤', 'surprised': '惊讶', 'neutral': '平静', 'unknown': '未知' };
        const translateEmotion = (key) => emotionTranslations[key] || '未知';

        // Emotion data for display (computed property for clarity)
        const currentEmotionData = computed(() => currentEmotion.value);
        const emotionDisplay = computed(() => translateEmotion(currentEmotion.value?.emotion || 'neutral'));


        // Update emotion data when detected by camera
        const updateEmotion = (data) => {
          // console.log("VoiceChatView received emotion data:", data);
          if (data) {
            // Only update if face was detected or explicitly set to unknown/neutral
            if (!data.no_face || ['neutral', 'unknown'].includes(data.emotion)) {
              currentEmotion.value = data;
            } else {
              // If no face, maybe clear the display or show 'no face'
              currentEmotion.value = null; // Clear if no face detected by camera
            }
          } else {
            currentEmotion.value = null; // Clear on null data
          }
        };

        // Handle camera status changes (e.g., log or update UI)
        const onCameraStatusChange = (isActive) => {
          console.log('摄像头状态变化 (View):', isActive ? '已开启' : '已关闭');
          if (!isActive) {
            // Optionally reset emotion display when camera stops
            // currentEmotion.value = null; 
          }
        };

        return {
          showEmotionPanel,
          currentEmotion, // Use the computed property name
          currentEmotionData,
          emotionDisplay,
          emotionCameraRef,
          updateEmotion,
          onCameraStatusChange,
          translateEmotion
        };
      }
    });

    // Register Components
    app.component('EmotionCamera', EmotionCameraComponent);
    app.component('VoiceChat', VoiceChatComponent);
    app.component('ElButton', ElButton); // Register Element Plus components used
    app.component('ElIcon', ElIcon);

    // Register Element Plus Icons Globally
    if (window.ElementPlusIconsVue) {
      for (const [key, component] of Object.entries(window.ElementPlusIconsVue)) {
        app.component(key, component);
      }
      console.log("Element Plus icons registered.");
    } else {
      console.error("Element Plus Icons Vue library not found.");
    }


    // Use Element Plus plugin
    app.use(ElementPlus);

    // Mount App
    app.mount('#app');

  </script>
</body>

</html>